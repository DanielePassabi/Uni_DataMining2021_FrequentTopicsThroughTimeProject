{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Test of algorithm scalability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from apyori import apriori as apriori_baseline\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MLXTEND APRIORI\n",
    "from mlxtend.frequent_patterns import apriori as mlxtend_apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/clean_df/clean_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tweets:  1074648\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['smell',\n",
       "  'scent',\n",
       "  'hand',\n",
       "  'sanit',\n",
       "  'today',\n",
       "  'someon',\n",
       "  'past',\n",
       "  'would',\n",
       "  'think',\n",
       "  'intox'],\n",
       " ['hey',\n",
       "  'yanke',\n",
       "  'yankeespr',\n",
       "  'mlb',\n",
       "  'made',\n",
       "  'sens',\n",
       "  'player',\n",
       "  'pay',\n",
       "  'respect'],\n",
       " ['dian',\n",
       "  'wdunlap',\n",
       "  'realdonaldtrump',\n",
       "  'trump',\n",
       "  'never',\n",
       "  'claim',\n",
       "  'covid',\n",
       "  'hoax',\n",
       "  'claim',\n",
       "  'effort'],\n",
       " ['brookbanktv',\n",
       "  'gift',\n",
       "  'covid',\n",
       "  'give',\n",
       "  'appreci',\n",
       "  'simpl',\n",
       "  'thing',\n",
       "  'alway',\n",
       "  'around']]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "texts_for_apriori = []\n",
    "for t in df[\"text\"]:\n",
    "    texts_for_apriori.append(t)\n",
    "\n",
    "# to reach over 1M tweets, we must duplicate the df at least 6 times\n",
    "texts_for_apriori = texts_for_apriori*6\n",
    "\n",
    "print(\"Total tweets: \", len(texts_for_apriori))\n",
    "texts_for_apriori[0:4]"
   ]
  },
  {
   "source": [
    "# Time measurament of APriori based on total number of tweets\n",
    "We run `n` times the algorithm, each time doubling the number of tweets. We keep track of the times of each run."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> [1000, 0.04387521743774414]\n",
      "> [2000, 0.14162588119506836]\n",
      "> [4000, 1.2596051692962646]\n",
      "> [8000, 2.7318878173828125]\n",
      "> [16000, 1.5887510776519775]\n",
      "> [32000, 3.8342647552490234]\n",
      "> [64000, 14.126312255859375]\n",
      "> [128000, 42.44559717178345]\n",
      "> [256000, 2044.1502013206482]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 49.9 GiB for an array with shape (512000, 104583) and data type bool",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6844a2e928d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransactionEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mte_ary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mte\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_for_apriori_custom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_for_apriori_custom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mmlxtend_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_ary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mte\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlxtend_apriori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlxtend_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_support\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\mlxtend\\preprocessing\\transactionencoder.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, sparse)\u001b[0m\n\u001b[0;32m    123\u001b[0m                                dtype=bool)\n\u001b[0;32m    124\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransaction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransaction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 49.9 GiB for an array with shape (512000, 104583) and data type bool"
     ]
    }
   ],
   "source": [
    "tweets_tot_number = [1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000, 256000, 512000, 1024000]\n",
    "min_support = 0.012\n",
    "mlxtend_times = []\n",
    "\n",
    "for i in range(len(tweets_tot_number)):\n",
    "\n",
    "    tot_tweets = tweets_tot_number[i]\n",
    "    texts_for_apriori_custom = texts_for_apriori[0:tot_tweets]\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(texts_for_apriori_custom).transform(texts_for_apriori_custom)\n",
    "    mlxtend_input = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    res = mlxtend_apriori(mlxtend_input, min_support=min_support)\n",
    "\n",
    "    end = time.time()\n",
    "    mlxtend_apriori_time = end - start\n",
    "    mlxtend_times.append([tot_tweets, mlxtend_apriori_time])\n",
    "\n",
    "    print(\">\", mlxtend_times[i])"
   ]
  },
  {
   "source": [
    "### Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_list = []\n",
    "times_list = []\n",
    "\n",
    "for i in range(len(mlxtend_times)):\n",
    "    tw_list.append(mlxtend_times[i][0])\n",
    "    times_list.append(mlxtend_times[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   number_of_tweets         time\n",
       "0              1000     0.043875\n",
       "1              2000     0.141626\n",
       "2              4000     1.259605\n",
       "3              8000     2.731888\n",
       "4             16000     1.588751\n",
       "5             32000     3.834265\n",
       "6             64000    14.126312\n",
       "7            128000    42.445597\n",
       "8            256000  2044.150201"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number_of_tweets</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000</td>\n      <td>0.043875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2000</td>\n      <td>0.141626</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4000</td>\n      <td>1.259605</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8000</td>\n      <td>2.731888</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16000</td>\n      <td>1.588751</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>32000</td>\n      <td>3.834265</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>64000</td>\n      <td>14.126312</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>128000</td>\n      <td>42.445597</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>256000</td>\n      <td>2044.150201</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "res_df = pd.DataFrame(\n",
    "    {'number_of_tweets': tw_list,\n",
    "     'time': times_list\n",
    "    })\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(\"../data/efficiency_and_scalability_results/scalability_res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}